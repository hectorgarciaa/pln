# ğŸ¤– Bot Negociador AutÃ³nomo â€” PLN

Bot autÃ³nomo que negocia intercambios de recursos en el servidor de juego **fdi-pln-butler**.  
Usa un modelo de IA local (Ollama) para analizar mensajes, detectar estafas, aceptar o rechazar ofertas y generar propuestas.

---

## ğŸ“‹ Ãndice

- [Requisitos](#-requisitos)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Inicio rÃ¡pido](#-inicio-rÃ¡pido)
- [Uso detallado](#-uso-detallado)
  - [main.py â€” Bot individual](#mainpy--bot-individual)
  - [test_runner.py â€” Orquestador multi-bot](#test_runnerpy--orquestador-multi-bot)
- [Arquitectura](#-arquitectura)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [LibrerÃ­as utilizadas](#-librerÃ­as-utilizadas)

---

## ğŸ”§ Requisitos

| Requisito | VersiÃ³n mÃ­nima |
|-----------|---------------|
| Python    | 3.12+         |
| [Ollama](https://ollama.com) | Cualquiera (con al menos un modelo descargado) |
| Servidor fdi-pln-butler | Corriendo en `http://127.0.0.1:7719` |

### Modelos de IA soportados

| Modelo | Velocidad | Notas |
|--------|-----------|-------|
| `llama3.2:3b` | âš¡âš¡âš¡ Ultra rÃ¡pido (3-5s) | Ligero |
| `phi3:mini` | âš¡âš¡âš¡ Muy rÃ¡pido (3-5s) | Ligero |
| `qwen3-vl:8b` | âš¡âš¡ Balance (5-10s) | Buena calidad |
| `qwen2.5:7b` | âš¡ Calidad (10-15s) | Muy preciso |
| `qwen3:8b` | â€” Solo texto | **Default**, con control de `<think>` |

---

## ğŸ“¦ InstalaciÃ³n

```bash
# Clonar o entrar en el directorio del proyecto
cd pln/

# Instalar dependencias con uv
uv sync
```

AsegÃºrate de tener Ollama corriendo y al menos un modelo descargado:

```bash
ollama pull qwen3:8b
ollama serve   # si no estÃ¡ corriendo ya
```

---

## ğŸš€ Inicio rÃ¡pido

```bash
# Desde la raÃ­z del proyecto:

# Modo interactivo (menÃº con opciones)
uv run app/main.py

# Lanzar un bot directo por lÃ­nea de comandos
uv run app/main.py --alias MiBot --debug

# Lanzar 3 bots en paralelo
uv run app/test_runner.py -n 3
```

---

## ğŸ“– Uso detallado

### `main.py` â€” Bot individual

Punto de entrada principal. Tiene dos modos:

#### Modo interactivo (sin flags)

```bash
uv run app/main.py
```

Abre un menÃº donde puedes:
1. **Iniciar agente autÃ³nomo** â€” configura modelo, debug, rondas y lanza el bot.
2. **Operaciones API manuales** â€” ver info, jugadores, enviar cartas/paquetes, etc.

#### Modo automÃ¡tico (con `--alias`)

```bash
uv run app/main.py --alias Bot_1 --modelo llama3.2:3b --debug --max-rondas 15 --pausa 20
```

Lanza el bot directamente sin menÃº interactivo.

#### Flags de `main.py`

| Flag | Corto | Default | DescripciÃ³n |
|------|-------|---------|-------------|
| `--alias` | `-a` | *(ninguno)* | Nombre del bot. **Si se da, modo automÃ¡tico.** |
| `--modelo` | `-m` | `qwen3:8b` | Modelo de IA a usar. |
| `--debug` | `-d` | `False` | Activa modo debug: muestra cada decisiÃ³n del agente (ğŸ“¤ğŸ“¥ğŸ”ğŸ§ ğŸ”„). |
| `--max-rondas` | `-r` | `10` | NÃºmero mÃ¡ximo de rondas de negociaciÃ³n. |
| `--pausa` | `-p` | `30` | Segundos de espera entre rondas (para dar tiempo a respuestas). |
| `--source-ip` | â€” | *(ninguno)* | IP local de origen para diferenciar jugadores en el butler. |
| `--api-url` | â€” | *(de config)* | URL base de la API del juego (por defecto `http://127.0.0.1:7719`). |
| `--help` | `-h` | â€” | Muestra la ayuda. |

#### Ejemplos

```bash
# Bot con modelo rÃ¡pido y debug
uv run app/main.py -a Bot_1 -m llama3.2:3b -d

# Bot silencioso, 20 rondas, pausa corta
uv run app/main.py -a Negociador -r 20 -p 10

# Bot conectado a otro servidor
uv run app/main.py -a Bot_1 --api-url http://192.168.1.100:7719
```

---

### `test_runner.py` â€” Orquestador multi-bot

Lanza N bots en paralelo, cada uno como un subproceso independiente de `main.py`.

```bash
uv run app/test_runner.py -n 5 --consola
```

#### Flags de `test_runner.py`

| Flag | Corto | Default | DescripciÃ³n |
|------|-------|---------|-------------|
| `-n` | â€” | `3` | NÃºmero de bots a lanzar. |
| `--prefijo` | â€” | `Bot` | Prefijo para los nombres (`Bot_1`, `Bot_2`, â€¦). |
| `--modelo` | `-m` | `qwen3:8b` | Modelo de IA para todos los bots. |
| `--debug / --no-debug` | `-d` | `True` | Activar/desactivar debug en los bots. |
| `--max-rondas` | `-r` | `10` | Rondas mÃ¡ximas por bot. |
| `--pausa` | `-p` | `30` | Pausa entre rondas (segundos). |
| `--consola` | â€” | `False` | Mostrar salida coloreada en terminal. Sin este flag, la salida va a archivos `logs/`. |
| `--help` | `-h` | â€” | Muestra la ayuda. |

#### Modos de salida

- **Modo logs (default):** cada bot escribe en `app/logs/Bot_1_20250101_120000.log`.
- **Modo consola (`--consola`):** salida coloreada en terminal, cada bot con un color distinto.

#### Cierre limpio

Pulsa `Ctrl+C` para detener todos los bots. El orquestador envÃ­a SIGTERM a cada proceso y muestra un resumen final.

---

## ğŸ—ï¸ Arquitectura

```
app/
â”œâ”€â”€ config.py          # ConfiguraciÃ³n centralizada (pydantic)
â”œâ”€â”€ api_client.py      # Cliente HTTP para la API del juego
â”œâ”€â”€ ollama_client.py   # Cliente para Ollama (IA local)
â”œâ”€â”€ negociador.py      # LÃ³gica del agente negociador
â”œâ”€â”€ main.py            # Punto de entrada (CLI con click)
â”œâ”€â”€ test_runner.py     # Orquestador multi-bot
â””â”€â”€ logs/              # Logs de ejecuciÃ³n (modo archivos)
```

### Flujo de datos

```
config.py
    â†“
api_client.py â†â†’ Servidor fdi-pln-butler (HTTP)
ollama_client.py â†â†’ Ollama (IA local)
    â†“
negociador.py  â† Usa ambos clientes
    â†“
main.py  â† Punto de entrada (1 bot)
test_runner.py  â† Lanza N bots (subprocesos de main.py)
```

### Loop del agente

Cada ronda el agente:

1. **Actualiza su estado** â€” consulta `/info` y `/gente`.
2. **Revisa el buzÃ³n** â€” analiza cada carta con IA:
   - Â¿Es estafa? â†’ Bloquear + lista negra.
   - Â¿Es aceptaciÃ³n? â†’ Ejecutar acuerdo pendiente (enviar paquete).
   - AnÃ¡lisis completo â†’ Aceptar / Contraofertar / Rechazar.
3. **EnvÃ­a propuestas** â€” contacta hasta 3 jugadores con ofertas estructuradas (`[OFREZCO]`/`[PIDO]`).
4. **Espera** â€” pausa configurable para dar tiempo a respuestas.
5. **Â¿Objetivo completado?** â†’ Cambia a modo *Maximizar Oro* o finaliza.

---

## âš™ï¸ ConfiguraciÃ³n

La configuraciÃ³n se gestiona en `app/config.py` con modelos **pydantic**. Los valores por defecto son:

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `api_base_url` | `http://127.0.0.1:7719` | URL del servidor del juego |
| `ollama_url` | `http://127.0.0.1:11434` | URL de Ollama |
| `modelo_default` | `qwen3:8b` | Modelo de IA por defecto |
| `think_timeout` | `25` | MÃ¡x. segundos de bloque `<think>` antes de cortar |
| `disable_think` | `False` | Forzar `/no_think` en modelos qwen3 |

### ParÃ¡metros de Ollama

| ParÃ¡metro | Valor | Efecto |
|-----------|-------|--------|
| `temperature` | `0.3` | Baja creatividad, respuestas mÃ¡s deterministas |
| `num_predict` | `100` | MÃ¡x. tokens de respuesta |
| `num_ctx` | `2048` | Ventana de contexto |
| `num_gpu` | `1` | Capas en GPU |
| `num_thread` | `8` | Hilos de CPU |

---

## ğŸ“š LibrerÃ­as utilizadas

| LibrerÃ­a | Uso |
|----------|-----|
| [requests](https://docs.python-requests.org/) | Cliente HTTP para la API del juego |
| [ollama](https://github.com/ollama/ollama-python) | Cliente oficial de Ollama (IA local) |
| [click](https://click.palletsprojects.com/) | CLI moderna (reemplaza argparse) |
| [rich](https://rich.readthedocs.io/) | Tablas, paneles, prompts y colores en terminal |
| [loguru](https://loguru.readthedocs.io/) | Logging estructurado (reemplaza print + logging) |
| [pydantic](https://docs.pydantic.dev/) | ValidaciÃ³n de configuraciÃ³n y respuestas de IA |
