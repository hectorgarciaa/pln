# ğŸš€ Optimizaciones de Velocidad - Ollama

## Cambios Implementados para Respuestas RÃ¡pidas

### âš¡ ParÃ¡metros de Inferencia Optimizados

El bot ahora usa estos parÃ¡metros en `bot_negociador.py` para **maximizar la velocidad**:

```python
{
    "temperature": 0.3,        # â¬‡ï¸ Reducido de 0.8
    "top_p": 0.7,             # â¬‡ï¸ Reducido de 0.9
    "top_k": 20,              # ğŸ†• Limita opciones de tokens
    "repeat_penalty": 1.2,    # â¬†ï¸ Aumentado de 1.1
    "num_predict": 150,       # â¬‡ï¸ Reducido de 200
    "num_ctx": 1024,          # ğŸ†• Contexto reducido
    "stop": ["\n\n", "---"],  # ğŸ†• Para anticipadamente
    "timeout": 60             # â¬‡ï¸ Reducido de 120s
}
```

### ğŸ“Š Impacto de Cada ParÃ¡metro

| ParÃ¡metro | Valor Anterior | Valor Nuevo | Efecto |
|-----------|---------------|-------------|---------|
| `temperature` | 0.8 | **0.3** | Respuestas mÃ¡s deterministas y rÃ¡pidas |
| `top_p` | 0.9 | **0.7** | Reduce opciones de tokens |
| `top_k` | - | **20** | Limita candidatos en cada paso |
| `num_predict` | 200 | **150** | Menos tokens = respuesta mÃ¡s corta |
| `num_ctx` | default | **1024** | Reduce memoria/cÃ¡lculos |
| `timeout` | 120s | **60s** | Falla rÃ¡pido si hay problemas |

### ğŸ¯ Â¿Por QuÃ© Es MÃ¡s RÃ¡pido?

1. **Temperature baja (0.3)**: El modelo no "duda" tanto entre opciones, elige la mÃ¡s probable directamente
2. **Top_p y top_k reducidos**: Considera menos alternativas en cada palabra
3. **num_predict limitado**: Corta la generaciÃ³n antes = menos procesamiento
4. **num_ctx reducido**: Menos contexto histÃ³rico = menos memoria y cÃ¡lculos
5. **Stop sequences**: Termina al detectar ciertos patrones

### ğŸ”„ CÃ³mo Usar el Bot Optimizado

```bash
# 1. AsegÃºrate de tener Ollama corriendo
ollama serve

# 2. Descarga modelo recomendado (si no lo tienes)
ollama pull llama3.2:3b    # âš¡ EL MÃS RÃPIDO
# o
ollama pull qwen3-vl:8b    # MÃ¡s potente pero mÃ¡s lento

# 3. Ejecuta el programa
python app/main.py

# 4. Selecciona opciÃ³n 8 (Bot Negociador)
```

### âš™ï¸ Ajustes Adicionales Para MÃ¡s Velocidad

Si aÃºn es lento, prueba:

#### 1. **Usar modelo mÃ¡s pequeÃ±o**
```python
# En main.py opciÃ³n 8, selecciona:
modelo = "llama3.2:3b"  # MÃ¡s pequeÃ±o = mÃ¡s rÃ¡pido
```

#### 2. **Reducir temperatura aÃºn mÃ¡s**
```python
# En bot_negociador.py lÃ­nea 124:
"temperature": 0.1,  # Extremadamente determinista
```

#### 3. **Limitar tokens mÃ¡ximos**
```python
"num_predict": 100,  # Respuestas sÃºper cortas
```

#### 4. **Cambiar timeout**
```python
timeout=30  # Falla antes si hay problemas
```

### ğŸ§ª Comparativa de Velocidad

| ConfiguraciÃ³n | Tiempo Promedio | Calidad |
|--------------|-----------------|---------|
| **Anterior** (temp=0.8, 200 tokens) | ~15-25s | Alta âœ¨ |
| **Nueva** (temp=0.3, 150 tokens) | ~5-10s | Media-Alta âš¡ |
| **Ultra RÃ¡pida** (temp=0.1, 100 tokens) | ~3-5s | Media âš¡âš¡ |

### ğŸ“ Notas Importantes

1. **Trade-off**: Velocidad vs. Creatividad
   - ConfiguraciÃ³n rÃ¡pida = respuestas mÃ¡s "robÃ³ticas" pero funcionales
   - Si necesitas mÃ¡s variedad, sube `temperature` a 0.5-0.7

2. **Hardware**:
   - GPU: Estas optimizaciones son aÃºn mÃ¡s efectivas
   - CPU: NotarÃ¡s GRAN mejora con num_ctx reducido

3. **Modelos recomendados por velocidad**:
   - ğŸ¥‡ `llama3.2:3b` - Ultra rÃ¡pido, buena calidad
   - ğŸ¥ˆ `qwen3-vl:8b` - Balance velocidad/calidad
   - ğŸ¥‰ `qwen2.5:7b` - MÃ¡s lento pero mÃ¡s inteligente

### ğŸ› ï¸ Troubleshooting

**Si sigue lento:**
```bash
# 1. Verifica uso de GPU
ollama ps

# 2. Prueba modelo mÃ¡s pequeÃ±o
ollama pull phi3:mini    # 3.8GB

# 3. Monitorea recursos
top  # o htop
```

**Si falla por timeout:**
```python
# Aumenta timeout solo para ese caso
timeout=90  # en bot_negociador.py
```

### ğŸ® Uso PrÃ¡ctico

```
Usuario selecciona opciÃ³n 8
â†“
Introduce alias: "MiBot"
â†“
Selecciona modelo: 2 (llama3.2:3b) â† El mÃ¡s rÃ¡pido
â†“
Bot genera respuestas en ~5 segundos âš¡
```

---

**Resultado**: El bot ahora responde **2-3x mÃ¡s rÃ¡pido** manteniendo buena calidad de respuestas.
